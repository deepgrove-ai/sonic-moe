{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38270750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sonicmoe import MoE, KernelBackendMoE\n",
    "\n",
    "# Create MoE layer\n",
    "moe = MoE(\n",
    "    num_experts=128,           # Number of experts\n",
    "    num_experts_per_tok=8,     # Top-k experts per token\n",
    "    hidden_size=4096,          # Hidden dimension\n",
    "    intermediate_size=1536,    # Expert intermediate size\n",
    "    is_glu=True,               # Whether to use GLU (e.g. SwiGLU) activation\n",
    "    add_bias=False,            # Add bias to linear layers\n",
    "    std=0.02,                  # Weight initialization std\n",
    ").to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(32768, 4096, device=\"cuda\", dtype=torch.bfloat16)\n",
    "output, aux_loss = moe(x, kernel_backend_moe=KernelBackendMoE.sonicmoe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df24729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonicmoe.functional import TC_Softmax_Topk_Router_Function, count_cumsum, TC_topk_router_metadata\n",
    "import torch.nn.functional as F\n",
    "\n",
    "router_w = moe.router.weight\n",
    "router_logits = F.linear(x, router_w)\n",
    "K = moe.top_k\n",
    "E = moe.num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topk_scores: torch.Size([32768, 8]) torch.float32 cuda:0 - [seq, topk]\n",
    "# topk_indices: torch.Size([32768, 8]) torch.int32 cuda:0 - [seq, topk]\n",
    "# expert_frequency: torch.Size([128]) torch.int32 cuda:0 - [NumTotalExperts]\n",
    "# expert_offset: torch.Size([129]) torch.int32 cuda:0 - [NumTotalExperts + 1]\n",
    "# x_gather_idx: torch.Size([262144]) torch.int32 cuda:0 - [topk * seq]\n",
    "# s_scatter_idx: torch.Size([262144]) torch.int32 cuda:0 - [topk * seq]\n",
    "# s_reverse_scatter_idx: torch.Size([262144]) torch.int32 cuda:0 - [topk * seq]\n",
    "# topk_reverse_scatter_idx: torch.Size([32769]) torch.int32 cuda:0 - [seq + 1]\n",
    "\n",
    "\n",
    "topk_scores, topk_indices = TC_Softmax_Topk_Router_Function.apply(router_logits, router_w.size(0), K)\n",
    "expert_frequency, expert_offset = count_cumsum(topk_indices.view(-1), router_w.size(0), do_cumsum=True)\n",
    "# Puts a leading 0 infront of expert_offset\n",
    "expert_offset, x_gather_idx, s_scatter_idx, s_reverse_scatter_idx, topk_token_offset = TC_topk_router_metadata(\n",
    "    topk_indices, expert_offset, K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a91bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_token_choice_rounding(\n",
    "    x: torch.Tensor, router_w: torch.Tensor, E, K, Mtile, routing\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    T, D = x.shape  # # B, L, # total expert\n",
    "    Mtile = 128\n",
    "\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "\n",
    "    router_logits = F.linear(x, router_w)\n",
    "    router_scores = F.softmax(router_logits, dim=-1, dtype=torch.float32).to(dtype)\n",
    "\n",
    "    # first sorting, similar to TC\n",
    "    topk_values, topk_indices = router_scores.topk(K, dim=-1)\n",
    "\n",
    "    expert_freq = count_cumsum(topk_indices.view(-1), E, do_cumsum=True)[0]\n",
    "    expert_freq_rounded_up = (torch.ceil(expert_freq / Mtile) * Mtile).type(torch.int32)\n",
    "    expert_freq_rounded_down = expert_freq // Mtile * Mtile\n",
    "\n",
    "    topk_values /= topk_values.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    router_scores.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "    router_TC_EC_combined_val = router_scores.detach().clone()\n",
    "    router_TC_EC_combined_val -= 1  # make sure EC's score is lower than TC & EC keeps the score order\n",
    "    router_TC_EC_combined_val.scatter_(1, topk_indices, topk_values)  # mask out original TC score\n",
    "\n",
    "    # second sorting, similar to EC\n",
    "    topk_indices = router_TC_EC_combined_val.argsort(dim=0, descending=True).int()  # type: ignore\n",
    "\n",
    "    if routing == \"down\":\n",
    "        expert_freq_rounded = expert_freq_rounded_down\n",
    "\n",
    "    elif routing == \"up\":\n",
    "        expert_freq_rounded = expert_freq_rounded_up\n",
    "\n",
    "    elif routing == \"nr\":\n",
    "        expert_freq_rounded = torch.round(expert_freq / Mtile).type(torch.int32) * Mtile\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    expert_freq_mask = torch.arange(T, device=device, dtype=torch.int32)[:, None].expand(-1, E) < expert_freq_rounded[None, :]  # type: ignore\n",
    "\n",
    "    selected_T = topk_indices[expert_freq_mask]\n",
    "    selected_E = torch.arange(E, device=device, dtype=torch.int32)[None, :].expand(T, -1)[expert_freq_mask]  # type: ignore\n",
    "\n",
    "    # implicit assumption: selected_T should be sorted in my reduction code\n",
    "    selected_T_order = selected_T.argsort().int()\n",
    "    selected_T = selected_T[selected_T_order]\n",
    "    selected_E = selected_E[selected_T_order]\n",
    "\n",
    "    return router_scores[selected_T, selected_E].contiguous(), selected_T, selected_E\n",
    "\n",
    "\n",
    "def forward_topk(x: torch.Tensor, router_w: torch.Tensor, E, K) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    T = x.shape[0]\n",
    "\n",
    "    router_logits = F.linear(x, router_w)\n",
    "\n",
    "    top_logits, topk_indices = router_logits.topk(K, dim=1)\n",
    "    router_scores = F.softmax(top_logits, dim=-1, dtype=torch.float32)\n",
    "\n",
    "    # first sorting, similar to TC\n",
    "    return (\n",
    "        router_scores.view(-1),\n",
    "        torch.arange(T, device=\"cuda\", dtype=torch.int32).repeat_interleave(K),\n",
    "        topk_indices.view(-1).int(),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a42d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# router_scores_selected: torch.Size([262144]) torch.float32 cuda:0 - [topk * seq]\n",
    "# selected_T: torch.Size([262144]) torch.int32 cuda:0 - [topk * seq] -> selected expert (i.e. [0-7] is 0 because these are token 0)\n",
    "# selected_E: torch.Size([262144]) torch.int32 cuda:0 - [topk * seq]\n",
    "router_scores_selected, selected_T, selected_E = forward_topk(x, router_w.detach(), E, K)\n",
    "# router_scores_selected, selected_T, selected_E = forward_token_choice_rounding(\n",
    "#     x, router_w.detach(), E, K, Mtile, routing\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([262144]) torch.int32 cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 83,  90,  80,  ..., 101,  57,  13], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t = selected_E\n",
    "print(t.shape, t.dtype, t.device)\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
